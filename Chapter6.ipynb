{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Logic Definition of Generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Show empirically that the information limit of 2 prediction bits per parameter also holds for nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension: 2, n: 80.2, 1.2468827930174564\n",
      "Dimension: 4, n: 72.4, 1.3812154696132595\n",
      "Dimension: 8, n: 72.2, 1.3850415512465373\n",
      "Dimension: 16, n: 56.1, 1.7825311942959001\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import mutual_info_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def fit_model(model, X, y, n):\n",
    "    model.fit(X[:n], y[:n])\n",
    "    pred_y = model.predict(X)\n",
    "    \n",
    "    return np.mean(pred_y == y)\n",
    "\n",
    "\n",
    "def test_function(n_samples, d):\n",
    "    X, y = make_classification(n_samples=n_samples, n_features=d, n_informative=d, n_redundant=0, n_classes=2)\n",
    "    \n",
    "    for i in range(1, n_samples):\n",
    "        acc = fit_model(KNeighborsClassifier(n_neighbors=1), X, y, i)\n",
    "        if acc > 0.999:\n",
    "            return i\n",
    "    \n",
    "    return -1\n",
    "            \n",
    "for i in range(1, 5):\n",
    "    d = 2 ** i\n",
    "    n_samples = 100\n",
    "    \n",
    "    n = []\n",
    "    for _ in range(10):\n",
    "        n.append(test_function(n_samples, d))\n",
    "    \n",
    "    n_avg = np.mean(n)\n",
    "    print(f\"Dimension: {d}, n: {n_avg}, {n_samples / n_avg}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the number is approaching 2 as the dimension increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Extend your experiments to multi-class classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension: 3, n: 75.2, 1.3297872340425532\n",
      "Dimension: 9, n: 86.8, 1.1520737327188941\n",
      "Dimension: 27, n: 81.9, 1.2210012210012209\n",
      "Dimension: 81, n: 78.15, 1.2795905310300704\n",
      "Dimension: 243, n: 68.0, 1.4705882352941178\n",
      "Dimension: 729, n: 58.45, 1.7108639863130881\n",
      "Dimension: 2187, n: 63.2, 1.5822784810126582\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import mutual_info_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def fit_model(model, X, y, n):\n",
    "    model.fit(X[:n], y[:n])\n",
    "    pred_y = model.predict(X)\n",
    "    \n",
    "    return np.mean(pred_y == y)\n",
    "\n",
    "\n",
    "def test_function(n_samples, d):\n",
    "    X, y = make_classification(n_samples=n_samples, n_features=d, n_informative=d, n_redundant=0, n_classes=3)\n",
    "    \n",
    "    for i in range(1, n_samples):\n",
    "        acc = fit_model(KNeighborsClassifier(n_neighbors=1), X, y, i)\n",
    "        if acc > 0.999:\n",
    "            return i\n",
    "    \n",
    "    return -1\n",
    "            \n",
    "for i in range(1, 8):\n",
    "    d = 3 ** i\n",
    "    n_samples = 100\n",
    "    \n",
    "    n = []\n",
    "    for _ in range(20):\n",
    "        n.append(test_function(n_samples, d))\n",
    "    \n",
    "    n_avg = np.mean(n)\n",
    "    print(f\"Dimension: {d}, n: {n_avg}, {n_samples / n_avg}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in the 3-class classification problem, we are converging to $3/(3-1) = 1.5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Finite State Machine Generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Implement a program that automatically creates a set of if-then clauses from the training table of a binary dataset of your choice. Implement different strategies to minimize the number of if-then clauses. Document your strategies, the number of resulting conditional clauses, and the accuracy achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rules: 4\n",
      "Accuracy: 0.65\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = (data.target == 1).astype(int)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[data.feature_names], df['target'], test_size=0.2, random_state=42)\n",
    "\n",
    "def extract_rules(X, y):\n",
    "    rules = []\n",
    "    for column in X.columns:\n",
    "        values = sorted(X[column].unique())\n",
    "        for value in values:\n",
    "            if y[X[column] <= value].mean() > 0.98:\n",
    "                rules.append((column, '<=', value))\n",
    "                break\n",
    "            elif y[X[column] <= value].mean() < 0.02:\n",
    "                rules.append((column, '>', value))\n",
    "                break\n",
    "    return rules\n",
    "\n",
    "rules = extract_rules(X_train, y_train)\n",
    "\n",
    "def apply_rules(X, rules):\n",
    "    correct = np.zeros(len(X))\n",
    "    for rule in rules:\n",
    "        column, operator, value = rule\n",
    "        if operator == '<=':\n",
    "            correct[X[column] > value] += 1\n",
    "        if operator == '>':\n",
    "            correct[X[column] <= value] += 1\n",
    "    \n",
    "    correct /= len(rules)\n",
    "    pred = (correct > 0.5).astype(int)\n",
    "    return pred\n",
    "\n",
    "predictions = apply_rules(X_train, rules)\n",
    "accuracy = (predictions == y_train).mean()\n",
    "print(f'Number of rules: {len(rules)}')\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I separate the dataset if more than 98% of one class has value greater than or smaller than the given value. The main strategy is using this 98% to try and filter out which rule is the best. In the end, I had 4 rules and an accuracy of 65%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Use the algorithms developed in (a) on different datasets. Again, observe how your choices make a difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rules: 4\n",
      "Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = (data.target == 3).astype(int)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[data.feature_names], df['target'], test_size=0.2, random_state=42)\n",
    "rules = extract_rules(X_train, y_train)\n",
    "predictions = apply_rules(X_train, rules)\n",
    "accuracy = (predictions == y_train).mean()\n",
    "print(f'Number of rules: {len(rules)}')\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given different datasets, the results are different. Some might be more easily separable, such as using a different type of flower in this case. Still using 4 rules, the accuracy becomes 100%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Finally, use the programs developed in (a) on a completely random dataset, generated artificially. Vary your strategies but also the number of input columns as well as the number of instances. How many if-then clauses do you need?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------2 columns, 100 instances-----\n",
      "Number of rules: 2\n",
      "Accuracy: 0.49\n",
      "------10 columns, 500 instances-----\n",
      "Number of rules: 10\n",
      "Accuracy: 0.49\n"
     ]
    }
   ],
   "source": [
    "print(\"------2 columns, 100 instances-----\")\n",
    "X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, n_classes=2)\n",
    "X = pd.DataFrame(X, columns=['x1', 'x2'])\n",
    "\n",
    "rules = extract_rules(X, y)\n",
    "predictions = apply_rules(X, rules)\n",
    "accuracy = (predictions == y).mean()\n",
    "print(f'Number of rules: {len(rules)}')\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "print(\"------10 columns, 500 instances-----\")\n",
    "X, y = make_classification(n_samples=500, n_features=10, n_informative=10, n_redundant=0, n_classes=2)\n",
    "X = pd.DataFrame(X, columns=[f'x{i}' for i in range(10)])\n",
    "\n",
    "rules = extract_rules(X, y)\n",
    "predictions = apply_rules(X, rules)\n",
    "accuracy = (predictions == y).mean()\n",
    "print(f'Number of rules: {len(rules)}')\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On generated artificial datasets, the accuracy is only around 50%. The number of rules I used is equivalent to the number of input columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Create a long random string using a Python program, and use a lossless compression algorithm of your choice to compress the string. Note the compression ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original string: 1000 bytes\n",
      "Compressed string: 748 bytes\n",
      "Compression ratio: 1.34\n"
     ]
    }
   ],
   "source": [
    "import zlib\n",
    "import random\n",
    "import string\n",
    "\n",
    "string_length = 1000\n",
    "original_string = ''.join(random.choice(string.ascii_letters) for _ in range(string_length))\n",
    "original_string = original_string.encode('ascii')\n",
    "compressed_string = zlib.compress(original_string)\n",
    "\n",
    "print(f'Original string: {len(original_string)} bytes')\n",
    "print(f'Compressed string: {len(compressed_string)} bytes')\n",
    "\n",
    "compression_ratio = len(original_string) / len(compressed_string)\n",
    "print(f'Compression ratio: {compression_ratio:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) What is the expected compression ratio in (a)? Explain why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected compression ratio should be 1 because if the string is truely random, you cannot find any patterns to compress down the information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
